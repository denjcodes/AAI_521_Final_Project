{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avk7zgEk-q79"
   },
   "source": [
    "## Part 0: Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChwFXPpE8vnx",
    "outputId": "eda0c7cc-4c62-41d3-ba54-037029f48b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Project Paths:\n",
      " Project Root: /content/drive/MyDrive/Colab Notebooks/MSAAI/AAI_521/Project\n",
      " Data Directory: /content/drive/MyDrive/Colab Notebooks/MSAAI/AAI_521/Project/data\n",
      " Models Directory: /content/drive/MyDrive/Colab Notebooks/MSAAI/AAI_521/Project/models\n",
      "downloading uv 0.9.11 x86_64-unknown-linux-gnu\n",
      "no checksums to verify\n",
      "installing to /usr/local/bin\n",
      "  uv\n",
      "  uvx\n",
      "everything's installed!\n",
      "uv 0.9.11\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#@title Step 1:  Mount Google drive, setup project paths, uv and dependancies\n",
    "import os\n",
    "from huggingface_hub import HfApi, list_repo_files\n",
    "from IPython.utils.text import list_strings\n",
    "from google.colab import files\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/Colab Notebooks/MSAAI/AAI_521/Project\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Project Paths:\")\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Data Directory: {DATA_DIR}\")\n",
    "print(f\" Models Directory: {MODELS_DIR}\")\n",
    "\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "!uv --version\n",
    "!uv pip install huggingface-hub --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovJ7hPqd_heY",
    "outputId": "b2b7ebbf-797b-4630-a8a8-b3e280ee8488"
   },
   "outputs": [],
   "source": [
    "#@title Step 2:  Check if Models exist on Hugging Face Hub\n",
    "\n",
    "REPO_ID = \"d2j666/asl-efficientnets\"\n",
    "REQUIRED_FILES = [\"efficientnetb4_asl.h5\", \"efficientnetb7_asl.h5\", \"efficientnetb9_asl.h5\"]\n",
    "SKIP_TRAINING = False\n",
    "\n",
    "print(\"Checking if models exist on Hugging Face Hub...\")\n",
    "\n",
    "try:\n",
    "  api = HfApi()\n",
    "  repo = api.repo_info(REPO_ID)\n",
    "  files = list_repo_files(repo_id=REPO_ID, repo_type=\"model\")\n",
    "\n",
    "  existing_models = [f for f in REQUIRED_FILES if f in files]\n",
    "  missing_models = [f for f in REQUIRED_FILES if f not in files]\n",
    "\n",
    "  if len(existing_models) == len(REQUIRED_FILES):\n",
    "    print(f\" All required models exist on Hugging Face Hub repository {REPO_ID}\")\n",
    "\n",
    "    for model in existing_models:\n",
    "      try:\n",
    "          print(f\"   Model {model} found.\")\n",
    "      except:\n",
    "          print(f\"   Model {model} not found.\")\n",
    "\n",
    "    SKIP_TRAINING = True\n",
    "    \n",
    "  elif len(existing_models) < len(REQUIRED_FILES):\n",
    "    print(f\"  Some models exist on Hugging Face Hub repository {REPO_ID}, but not all required models.\")\n",
    "    print(f\"   Existing models: {existing_models}\")\n",
    "    print(f\"   Missing models: {missing_models}\")\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e).lower()\n",
    "    SKIP_TRAINING = False\n",
    "\n",
    "print(f\"\\n Skip Training? {SKIP_TRAINING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0sl72fmAl3x",
    "outputId": "09dd4449-4ae1-43be-b984-4858e0ff59d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#@title Step 2:  Check GPU Availability\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import EfficientNetB4, EfficientNetB7, EfficientNetV2L\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "  raise Exception(\"Go to Runtime â†’ Change runtime type â†’ Select GPU (A100 if available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rj4isUHVDt0m",
    "outputId": "b18dd3fb-d340-484f-b4c4-1afd14be656a"
   },
   "outputs": [],
   "source": "#@title Step 3: Download Kaggle dataset directly to Colab\nfrom google.colab import files\n\n# Check if kaggle.json already exists\nkaggle_dir = pathlib.Path.home() / \".kaggle\"\nkaggle_json_path = kaggle_dir / \"kaggle.json\"\n\nif not kaggle_json_path.exists():\n  print(\"ðŸ“‹ Kaggle credentials not found\")\n  print(\"   Please upload your kaggle.json file:\\n\")\n  uploaded = files.upload()\n\n  if \"kaggle.json\" not in uploaded:\n    raise Exception(\"âŒ Could not upload kaggle.json\")\n  else:\n    kaggle_dir.mkdir(exist_ok=True)\n    shutil.move(\"kaggle.json\", kaggle_json_path)\n    os.chmod(kaggle_json_path, 0o600)\n    print(\"âœ… Kaggle credentials configured!\")\nelse:\n  print(\"âœ… Kaggle credentials already exist\")\n\nprint(\"\\nðŸ” Testing Kaggle API connection...\")\n!kaggle datasets list -s \"asl alphabet\" | head -3\n\n# Define paths\nLOCAL_DATA_PATH = \"/content/asl_alphabet\"\nTRAIN_DATA_PATH = os.path.join(LOCAL_DATA_PATH, \"asl_alphabet_train\", \"asl_alphabet_train\")\nTEST_DATA_PATH = os.path.join(LOCAL_DATA_PATH, \"asl_alphabet_test\", \"asl_alphabet_test\")\n\n# Download data directly to Colab local storage (fastest option)\nif os.path.exists(TRAIN_DATA_PATH) and len(os.listdir(TRAIN_DATA_PATH)) == 29:\n  print(f\"\\nâœ… Dataset already in Colab local storage\")\n  print(f\"   Classes found: {len(os.listdir(TRAIN_DATA_PATH))}\")\nelse:\n  print(\"\\nðŸ“¥ Downloading ASL dataset from Kaggle to Colab...\")\n  print(\"   Size: ~1.1 GB | Extracting directly to local storage\")\n  print(\"   âš¡ Direct download is faster than copying from Drive\\n\")\n  !kaggle datasets download -d grassknoted/asl-alphabet -p {LOCAL_DATA_PATH} --unzip\n  print(\"\\nâœ… Dataset downloaded and extracted!\")\n\nprint(f\"\\nðŸ“Š Dataset Ready:\")\nprint(f\"   Train path: {TRAIN_DATA_PATH}\")\nprint(f\"   Test path: {TEST_DATA_PATH}\")\nprint(f\"   Classes: {os.listdir(TRAIN_DATA_PATH)[:10]} ... (29 total)\")\n\n# Optional: Copy to Drive for backup (uncomment if needed)\n# ASL_DATA_ROOT = os.path.join(DATA_DIR, \"asl_alphabet\")\n# if not os.path.exists(ASL_DATA_ROOT):\n#   print(f\"\\nðŸ’¾ Backing up dataset to Drive...\")\n#   shutil.copytree(LOCAL_DATA_PATH, ASL_DATA_ROOT)\n#   print(f\"   âœ… Saved to: {ASL_DATA_ROOT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DGhKIoR3HO0"
   },
   "source": [
    "## Part 1:  Pipeline Setup, Model Building, Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MupxvTV3F61",
    "outputId": "657d529f-c12f-49bb-e0cb-c5e8189f175e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 87000 files belonging to 29 classes.\n",
      "Using 69600 files for training.\n",
      "Found 87000 files belonging to 29 classes.\n",
      "Using 17400 files for validation.\n",
      "\n",
      " Data pipeline ready!\n",
      "  Training batches: 1088\n",
      "  Validation batches: 272\n",
      "  Classes: ['A', 'B', 'C', 'D', 'E'] ... (29 total)\n"
     ]
    }
   ],
   "source": [
    "#@title Step 1:  Setup Data Generators\n",
    "# Configuration\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 64  # Use 32 if GPU memory limited\n",
    "NUM_CLASSES = 29\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Create datasets using modern API\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  TRAIN_DATA_PATH,\n",
    "  label_mode=\"categorical\",\n",
    "  image_size=IMG_SIZE,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  TRAIN_DATA_PATH,\n",
    "  label_mode=\"categorical\",\n",
    "  image_size=IMG_SIZE,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=42\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "# Performance optimizations\n",
    "train_ds = train_ds.shuffle(1000).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal\"),\n",
    "  layers.RandomRotation(0.05),      # ~18 degrees\n",
    "  layers.RandomZoom(0.1),\n",
    "  layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "print(f\"\\n Data pipeline ready!\")\n",
    "print(f\"  Training batches: {len(train_ds)}\")\n",
    "print(f\"  Validation batches: {len(val_ds)}\")\n",
    "print(f\"  Classes: {class_names[:5]} ... ({NUM_CLASSES} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMtz9oaw4jGu"
   },
   "outputs": [],
   "source": [
    "#@title Step 1a: Optional callbacks\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(\"   Available: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\")\n",
    "print(\"   Note: Not used by default, but ready if you want to add them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOFARbUk33Ma"
   },
   "outputs": [],
   "source": [
    "#@title Step 2:  Build EfficientNet Model\n",
    "def build_efficientnet_model(model_name=\"B4\", num_classes=29):\n",
    "  \"\"\"\n",
    "  Build EfficientNet model with 2-stage training capability.\n",
    "\n",
    "  Args:\n",
    "      model_name: \"B4\", \"B7\", or \"B9\"\n",
    "      num_classes: Number of output classes (29 for ASL)\n",
    "\n",
    "  Returns:\n",
    "      Tuple of (model, base_model) ready for 2-stage training\n",
    "  \"\"\"\n",
    "  print(f\"\\n{'='*60}\")\n",
    "  print(f\"Building EfficientNet{model_name} model...\")\n",
    "  print(f\"{'='*60}\")\n",
    "\n",
    "  # Load pre-trained base model\n",
    "  if model_name == \"B4\":\n",
    "      base_model = EfficientNetB4(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=IMG_SIZE + (3,)\n",
    "      )\n",
    "  elif model_name == \"B7\":\n",
    "      base_model = EfficientNetB7(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=IMG_SIZE + (3,)\n",
    "      )\n",
    "  elif model_name == \"B9\":\n",
    "      base_model = EfficientNetV2L(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=IMG_SIZE + (3,)\n",
    "      )\n",
    "  else:\n",
    "      raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "  # Build model with augmentation\n",
    "  inputs = layers.Input(shape=IMG_SIZE + (3,))\n",
    "  x = data_augmentation(inputs)\n",
    "  x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
    "  x = base_model(x, training=False)\n",
    "  x = layers.GlobalAveragePooling2D()(x)\n",
    "  x = layers.Dropout(0.3)(x)\n",
    "  outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "  model = models.Model(inputs, outputs)\n",
    "\n",
    "  print(f\"\\n Model built successfully!\")\n",
    "  print(f\"   Total parameters: {model.count_params():,}\")\n",
    "\n",
    "  return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-39vFbnr5iRp",
    "outputId": "885bc2bb-8fa7-4ca2-8a01-0a74652a82ba"
   },
   "outputs": [],
   "source": [
    "#@title Step 3: Override SKIP_TRAINING (Optional - Set to False to force training)\n",
    "# Uncomment the line below to force training even if models exist\n",
    "# SKIP_TRAINING = False\n",
    "\n",
    "print(f\" Skip Training? {SKIP_TRAINING}\")\n",
    "if SKIP_TRAINING:\n",
    "  print(\"    Training will be skipped (models already exist)\")\n",
    "  print(\"    To force retraining, uncomment the line above\")\n",
    "else:\n",
    "  print(\"   ðŸ”„ Training will proceed\")\n",
    "  print(\"   â±ï¸  Expected time: ~2 hours with GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcEe8kFe5PAQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exHjbjfr4wfY"
   },
   "outputs": [],
   "source": [
    "#@title Step 3a: Train EfficientNetB4 (2-Stage Fine-Tuning)\n",
    "# Stage 1: Freeze backbone, train head (5 epochs)\n",
    "# Stage 2: Fine-tune top layers (5 epochs)\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"â­ï¸  SKIPPING EFFICIENTNETB4 TRAINING\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"\\n Models already exist in HuggingFace Hub\")\n",
    "  print(\" Run Step 17A to download them to your Drive\")\n",
    "else:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB4 - STAGE 1: WARM-UP\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  # Build model\n",
    "  model_b4, base_b4 = build_efficientnet_model(\"B4\", num_classes=NUM_CLASSES)\n",
    "\n",
    "  # Stage 1: Freeze backbone, train classification head\n",
    "  base_b4.trainable = False\n",
    "\n",
    "  model_b4.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 1\n",
    "  history_b4_stage1 = model_b4.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "\n",
    "  print(f\"\\n Stage 1 complete! Val accuracy: {history_b4_stage1.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "  # Stage 2: Fine-tune top layers\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB4 - STAGE 2: FINE-TUNING\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  base_b4.trainable = True\n",
    "\n",
    "  # Freeze lower layers, train top ~150 layers\n",
    "  for layer in base_b4.layers[:-150]:\n",
    "      layer.trainable = False\n",
    "\n",
    "  model_b4.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate!\n",
    "      loss=\"categorical_crossentropy\",\n",
    "      metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 2\n",
    "  history_b4_stage2 = model_b4.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=5\n",
    "  )\n",
    "\n",
    "  # Save model\n",
    "  b4_save_path = os.path.join(MODELS_DIR, 'efficientnetb4_asl.h5')\n",
    "  model_b4.save(b4_save_path)\n",
    "\n",
    "  best_val_acc = max(history_b4_stage2.history['val_accuracy'])\n",
    "  print(f\"\\n EfficientNetB4 training complete!\")\n",
    "  print(f\"   Saved to: {b4_save_path}\")\n",
    "  print(f\"   Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfZx2BFC8mHx"
   },
   "outputs": [],
   "source": [
    "#@title Step 3b: Train EfficientNetB7 (2-Stage Fine-Tuning)\n",
    "# Stage 1: Freeze backbone, train head (5 epochs)\n",
    "# Stage 2: Fine-tune top layers (5 epochs)\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"â­ï¸  SKIPPING EFFICIENTNETB7 TRAINING\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"\\n Models already exist in HuggingFace Hub\")\n",
    "else:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB7 - STAGE 1: WARM-UP\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  # Build model\n",
    "  model_b7, base_b7 = build_efficientnet_model(\"B7\", num_classes=NUM_CLASSES)\n",
    "\n",
    "  # Stage 1: Freeze backbone, train classification head\n",
    "  base_b7.trainable = False\n",
    "\n",
    "  model_b7.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 1\n",
    "  history_b7_stage1 = model_b7.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "\n",
    "  print(f\"\\n Stage 1 complete! Val accuracy: {history_b7_stage1.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "  # Stage 2: Fine-tune top layers\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB7 - STAGE 2: FINE-TUNING\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  base_b7.trainable = True\n",
    "\n",
    "  # Freeze lower layers, train top ~150 layers\n",
    "  for layer in base_b7.layers[:-150]:\n",
    "      layer.trainable = False\n",
    "\n",
    "  model_b7.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate!\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 2\n",
    "  history_b7_stage2 = model_b7.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "\n",
    "  # Save model\n",
    "  b7_save_path = os.path.join(MODELS_DIR, 'efficientnetb7_asl.h5')\n",
    "  model_b7.save(b7_save_path)\n",
    "\n",
    "  best_val_acc = max(history_b7_stage2.history['val_accuracy'])\n",
    "  print(f\"\\n EfficientNetB7 training complete!\")\n",
    "  print(f\"   Saved to: {b7_save_path}\")\n",
    "  print(f\"   Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXBVQSHa71TC"
   },
   "outputs": [],
   "source": [
    "#@title Step 3c: Train EfficientNetB9 (2-Stage Fine-Tuning)\n",
    "# Stage 1: Freeze backbone, train head (5 epochs)\n",
    "# Stage 2: Fine-tune top layers (5 epochs)\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"â­ï¸  SKIPPING EFFICIENTNETB9 TRAINING\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"\\n Models already exist in HuggingFace Hub\")\n",
    "else:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB9 - STAGE 1: WARM-UP\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  # Build model\n",
    "  model_b9, base_b9 = build_efficientnet_model(\"B9\", num_classes=NUM_CLASSES)\n",
    "\n",
    "  # Stage 1: Freeze backbone, train classification head\n",
    "  base_b9.trainable = False\n",
    "\n",
    "  model_b9.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 1\n",
    "  history_b9_stage1 = model_b9.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "\n",
    "  print(f\"\\n Stage 1 complete! Val accuracy: {history_b9_stage1.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "  # Stage 2: Fine-tune top layers\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"TRAINING EFFICIENTNETB9 - STAGE 2: FINE-TUNING\")\n",
    "  print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "  base_b9.trainable = True\n",
    "\n",
    "  # Freeze lower layers, train top ~150 layers\n",
    "  for layer in base_b9.layers[:-150]:\n",
    "      layer.trainable = False\n",
    "\n",
    "  model_b9.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate!\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    "  )\n",
    "\n",
    "  # Train Stage 2\n",
    "  history_b9_stage2 = model_b9.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "\n",
    "  # Save model\n",
    "  b9_save_path = os.path.join(MODELS_DIR, 'efficientnetb9_asl.h5')\n",
    "  model_b9.save(b9_save_path)\n",
    "\n",
    "  best_val_acc = max(history_b9_stage2.history['val_accuracy'])\n",
    "  print(f\"\\n EfficientNetB9 training complete!\")\n",
    "  print(f\"   Saved to: {b9_save_path}\")\n",
    "  print(f\"   Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ihz7b3hp-KY-"
   },
   "outputs": [],
   "source": [
    "#@title Step 4: Training Results Comparison\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n No training occurred (models already exist)\")\n",
    "  print(\"   Models are available on HuggingFace Hub\")\n",
    "else:\n",
    "  import matplotlib.pyplot as plt\n",
    "  import pandas as pd\n",
    "\n",
    "  def combine_histories(h1, h2):\n",
    "    combined = {}\n",
    "    for key in h1.history.keys():\n",
    "      combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "  history_b4_full = combine_histories(history_b4_stage1, history_b4_stage2)\n",
    "  history_b7_full = combine_histories(history_b7_stage1, history_b7_stage2)\n",
    "  history_b9_full = combine_histories(history_b9_stage1, history_b9_stage2)\n",
    "\n",
    "  results = {\n",
    "      \"Model\": [\"EfficientNetB4\", \"EfficientNetB7\", \"EfficientNetB9\"],\n",
    "      \"Stage 1 Val Acc\": [\n",
    "          f\"{history_b4_stage1.history['val_accuracy'][-1]:.4f}\",\n",
    "          f\"{history_b7_stage1.history['val_accuracy'][-1]:.4f}\",\n",
    "          f\"{history_b9_stage1.history['val_accuracy'][-1]:.4f}\"\n",
    "      ],\n",
    "      \"Final Val Acc\": [\n",
    "          f\"{history_b4_full['val_accuracy'][-1]:.4f}\",\n",
    "          f\"{history_b7_full['val_accuracy'][-1]:.4f}\",\n",
    "          f\"{history_b9_full['val_accuracy'][-1]:.4f}\"\n",
    "      ],\n",
    "      \"Best Val Acc\": [\n",
    "          f\"{max(history_b4_full['val_accuracy']):.4f}\",\n",
    "          f\"{max(history_b7_full['val_accuracy']):.4f}\",\n",
    "          f\"{max(history_b9_full['val_accuracy']):.4f}\"\n",
    "      ]\n",
    "  }\n",
    "\n",
    "  df_results = pd.DataFrame(results)\n",
    "  print(\"\\n\" + \"=\"*70)\n",
    "  print(\"MODEL COMPARISON SUMMARY (2-STAGE TRAINING)\")\n",
    "  print(\"=\"*70)\n",
    "  print(df_results.to_string(index=False))\n",
    "  print(\"=\"*70)\n",
    "\n",
    "  # Plot training curves\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "  for idx, (history, name) in enumerate([\n",
    "    (history_b4_full, \"EfficientNetB4\"),\n",
    "    (history_b7_full, \"EfficientNetB7\"),\n",
    "    (history_b9_full, \"EfficientNetB9\")\n",
    "  ]):\n",
    "    ax = axes[idx]\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "    ax.plot(epochs, history['accuracy'], label='Train Acc', linewidth=2)\n",
    "    ax.plot(epochs, history['val_accuracy'], label='Val Acc', linewidth=2)\n",
    "    ax.axvline(x=5, color='red', linestyle='--', label='Stage 1â†’2')\n",
    "    ax.set_title(f\"{name}\\n2-Stage Training\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  chart_path = os.path.join(MODELS_DIR, 'training_comparison.png')\n",
    "  plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
    "  print(f\"\\n Comparison chart saved to: {chart_path}\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQ366j9G_rgD"
   },
   "outputs": [],
   "source": [
    "#@title Step 5: Test Models with Sample Predictions\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\"  SKIPPING MODEL TESTING\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"\\n  Models were not trained in this session\")\n",
    "  print(\" To test predictions:\")\n",
    "  print(\"   1. Set SKIP_TRAINING = False and run training cells, OR\")\n",
    "  print(\"   2. Load models from Drive/HuggingFace Hub first\")\n",
    "else:\n",
    "  def test_prediction(model, img_path, model_name):\n",
    "      \"\"\"Test a single image prediction.\"\"\"\n",
    "      img = image.load_img(img_path, target_size=(224, 224))\n",
    "      img_array = image.img_to_array(img)\n",
    "      img_array = preprocess_input(img_array)\n",
    "      img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "      preds = model.predict(img_array, verbose=0)[0]\n",
    "      top_idx = np.argmax(preds)\n",
    "\n",
    "      print(f\"{model_name:20} â†’ {class_names[top_idx]:10} (confidence: {preds[top_idx]:.3f})\")\n",
    "\n",
    "  print(\"Testing Model Predictions\")\n",
    "\n",
    "  for test_num in range(3):\n",
    "      test_class = random.choice(class_names)\n",
    "      test_class_dir = os.path.join(TRAIN_DATA_PATH, test_class)\n",
    "      test_img = random.choice(os.listdir(test_class_dir))\n",
    "      test_img_path = os.path.join(test_class_dir, test_img)\n",
    "\n",
    "      print(f\"Test {test_num + 1}: True label = '{test_class}'\")\n",
    "      print(\"-\" * 60)\n",
    "\n",
    "      test_prediction(model_b4, test_img_path, \"EfficientNetB4\")\n",
    "      test_prediction(model_b7, test_img_path, \"EfficientNetB7\")\n",
    "      test_prediction(model_b9, test_img_path, \"EfficientNetB9\")\n",
    "\n",
    "      print(\"\\nSample image:\")\n",
    "      display(Image.open(test_img_path).resize((150, 150)))\n",
    "      print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKZjLxY-B90F",
    "outputId": "c1748ec7-6578-4af5-cd3d-391934270248"
   },
   "outputs": [],
   "source": [
    "#@title Step 6: Upload Models to HuggingFace Hub\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "  print(\"\\n\" + \"=\"*60)\n",
    "  print(\" SKIPPING MODEL UPLOAD\")\n",
    "  print(\"=\"*60)\n",
    "  print(\"\\n  No models to upload (training was skipped)\")\n",
    "  print(f\"    Models already exist at: https://huggingface.co/{REPO_ID}\")\n",
    "else:\n",
    "  print(\"\\n Please ensure your HuggingFace token is set as a Colab secret (HF_TOKEN)\")\n",
    "  print(\"    Get it from: https://huggingface.co/settings/tokens (type: Write)\\n\")\n",
    "\n",
    "  api = HfApi()\n",
    "  REPO_ID = \"d2j666/asl-efficientnets\"\n",
    "  \n",
    "  try:\n",
    "      api.create_repo(repo_id=REPO_ID, exist_ok=True, private=True)\n",
    "      print(f\" Repository '{REPO_ID}' created or already exists.\")\n",
    "  except Exception as e:\n",
    "      print(f\" An error occurred: {e}\")\n",
    "  \n",
    "  print(f\"\\n Uploading models to: {REPO_ID}\\n\")\n",
    "\n",
    "  model_files = [\n",
    "      (os.path.join(MODELS_DIR, 'efficientnetb4_asl.h5'), 'efficientnetb4_asl.h5'),\n",
    "      (os.path.join(MODELS_DIR, 'efficientnetb7_asl.h5'), 'efficientnetb7_asl.h5'),\n",
    "      (os.path.join(MODELS_DIR, 'efficientnetb9_asl.h5'), 'efficientnetb9_asl.h5')\n",
    "  ]\n",
    "\n",
    "  upload_count = 0\n",
    "  for local_path, repo_path in model_files:\n",
    "      if os.path.exists(local_path):\n",
    "          file_size_mb = os.path.getsize(local_path) / (1024 * 1024)\n",
    "          print(f\" Uploading {repo_path} ({file_size_mb:.1f} MB)...\")\n",
    "\n",
    "          try:\n",
    "              api.upload_file(\n",
    "                  path_or_fileobj=local_path,\n",
    "                  path_in_repo=repo_path,\n",
    "                  repo_id=REPO_ID,\n",
    "                  repo_type=\"model\"\n",
    "              )\n",
    "              print(f\"    {repo_path} uploaded!\\n\")\n",
    "              upload_count += 1\n",
    "          except Exception as e:\n",
    "              print(f\"    Error: {e}\\n\")\n",
    "      else:\n",
    "          print(f\"  {repo_path} not found at {local_path}\\n\")\n",
    "\n",
    "  if upload_count == len(model_files):\n",
    "      print(\" ALL MODELS UPLOADED!\")\n",
    "  else:\n",
    "      print(f\"  PARTIAL UPLOAD ({upload_count}/{len(model_files)} successful)\")\n",
    "  print(f\"\\n View at: https://huggingface.co/{REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZEfKlhlCODP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}